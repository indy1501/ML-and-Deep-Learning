{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow Recommendation - Matrix Factorization.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/indy1501/ML-and-Deep-Learning/blob/main/TensorFlow_Recommendation_Matrix_Factorization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f-reQ11gbLB"
      },
      "source": [
        "In this tutorial, we build a simple matrix factorization model using the [MovieLens 100K dataset](https://grouplens.org/datasets/movielens/100k/) with TFRS. We can use this model to recommend movies for a given user."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jeQo5Z21y54A"
      },
      "source": [
        "Tutorial: https://www.tensorflow.org/recommenders/examples/quickstart"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA00wBE2Ntdm"
      },
      "source": [
        "### Import TFRS\n",
        "\n",
        "First, install and import TFRS:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6yzAaM85Z12D"
      },
      "source": [
        "!pip install -q tensorflow-recommenders #https://github.com/tensorflow/recommenders\n",
        "!pip install -q --upgrade tensorflow-datasets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3oYt3R6Nr9l"
      },
      "source": [
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xA97JEtStzN",
        "outputId": "1b1bc97a-89c0-4e5c-d126-25542090cf64"
      },
      "source": [
        "!pip install scann #we can use the scann package. This is an optional dependency of TFRS"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scann\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/14/ddc441a359e9947bb25befb86ec9c6f47d2d45cce7776ae20237cf9fd08d/scann-1.1.1-cp36-cp36m-manylinux2014_x86_64.whl (11.7MB)\n",
            "\u001b[K     |████████████████████████████████| 11.7MB 4.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scann) (2.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scann) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (0.2.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (0.10.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (1.1.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (0.35.1)\n",
            "Requirement already satisfied: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (1.4.1)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (3.12.4)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (0.3.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (1.33.2)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (2.3.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (1.15.0)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (2.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (1.6.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow~=2.3.0->scann) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow~=2.3.0->scann) (50.3.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (1.17.2)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (1.7.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (0.4.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (3.3.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (2.23.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (4.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (2020.11.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (2.10)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow~=2.3.0->scann) (3.4.0)\n",
            "Installing collected packages: scann\n",
            "Successfully installed scann-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCxQ1CZcO2wh"
      },
      "source": [
        "### Read the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYTSvKSC-gil"
      },
      "source": [
        "We use the MovieLens dataset from Tensorflow Datasets. Loading movielens/100k_ratings yields a tf.data.Dataset object containing the ratings data and loading movielens/100k_movies yields a tf.data.Dataset object containing only the movies data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jknFUZNxyc-Q"
      },
      "source": [
        "# Ratings data.\n",
        "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
        "# Features of all the available movies.\n",
        "movies = tfds.load('movielens/100k-movies', split=\"train\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR-D6fWt3vlq"
      },
      "source": [
        "!ls /root/tensorflow_datasets/movielens/100k-movies/0.1.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUFc7M283zPD"
      },
      "source": [
        "ratings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNlueGsI-0N4"
      },
      "source": [
        "The ratings dataset returns a dictionary of movie id, user id, the assigned rating, timestamp, movie information, and user information:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1Gi4Mfv-00i"
      },
      "source": [
        "import pprint #Data pretty printer: https://docs.python.org/3/library/pprint.html\n",
        "for x in ratings.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rkc1x8x4BOm"
      },
      "source": [
        "movies"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KuSRQJTO_NIR"
      },
      "source": [
        "The movies dataset contains the movie id, movie title, and data on what genres it belongs to. Note that the genres are encoded with integer labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fw1DU_6J_N1n"
      },
      "source": [
        "for x in movies.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUB26V2teVvO"
      },
      "source": [
        "# Retrieval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M-mxBYjdO5m7"
      },
      "source": [
        "# Select the basic features, We keep only the user_id, and movie_title fields in the dataset.\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"]\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZs19k9Lefy_"
      },
      "source": [
        "#use a random split, putting 80% of the ratings in the train set, and 20% in the test set.\n",
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx64Aj3bepRa"
      },
      "source": [
        "map the raw values of our categorical features to embedding vectors in our models. To do that, we need a vocabulary that maps a raw feature value to an integer in a contiguous range: this allows us to look up the corresponding embeddings in our embedding tables."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS7g7xd0_wqI"
      },
      "source": [
        "movie_titles = movies.batch(1_000) #The underscores have no semantic meaning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEUsrVnkAL8l"
      },
      "source": [
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rnod05-hAWIL"
      },
      "source": [
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odjli7QDAiT_"
      },
      "source": [
        "unique_movie_titles[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRCFl-TFAlcl"
      },
      "source": [
        "unique_user_ids[:20]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aJF7DgiBAuc"
      },
      "source": [
        "## Implementing a model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFen9f2pe4S8"
      },
      "source": [
        "we are building a two-tower retrieval model, we can build each tower separately and then combine them in the final model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwLQFORtBEEi"
      },
      "source": [
        "## The Query tower"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcQvelisBF4f"
      },
      "source": [
        "#The first step is to decide on the dimensionality of the query and candidate representations:\n",
        "embedding_dimension = 32"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5nFLzZsBQIq"
      },
      "source": [
        "Here, we're going to use Keras preprocessing layers to first convert user ids to integers, and then convert those to user embeddings via an Embedding layer. Note that we use the list of unique user ids we computed earlier as a vocabulary:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOMpVTulBPaZ"
      },
      "source": [
        "user_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "      vocabulary=unique_user_ids, mask_token=None),\n",
        "  # We add an additional embedding to account for unknown tokens.\n",
        "  tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcobMwtWfBqt"
      },
      "source": [
        "A simple model like this corresponds exactly to a classic matrix factorization approach. Return an embedding_dimension-wide output at the end"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FYY0y5jxBh-m"
      },
      "source": [
        "## The candidate tower"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4-oGEAtBhWI"
      },
      "source": [
        "movie_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "      vocabulary=unique_movie_titles, mask_token=None),\n",
        "  tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J8CzQkURVcwQ"
      },
      "source": [
        "## Metrics\n",
        "In our training data we have positive (user, movie) pairs. To figure out how good our model is, we need to compare the affinity score that the model calculates for this pair to the scores of all the other possible candidates: if the score for the positive pair is higher than for all other candidates, our model is highly accurate.\n",
        "To do this, we can use the tfrs.metrics.FactorizedTopK metric. The metric has one required argument: the dataset of candidates that are used as implicit negatives for evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMacuDE_Vmkz"
      },
      "source": [
        "#the movies dataset, converted into embeddings via our movie model:\n",
        "metrics = tfrs.metrics.FactorizedTopK(\n",
        "  candidates=movies.batch(128).map(movie_model)\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLItqzJHXAcI"
      },
      "source": [
        "#we'll make use of the Retrieval task object: a convenience wrapper that bundles together the loss function and metric computation:\n",
        "task = tfrs.tasks.Retrieval(\n",
        "  metrics=metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TwHs3cUXJEx"
      },
      "source": [
        "The task itself is a Keras layer that takes the query and candidate embeddings as arguments, and returns the computed loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wnM0mknpV8Y7"
      },
      "source": [
        "## The full model\n",
        "TFRS exposes a base model class (tfrs.models.Model) which streamlines bulding models: all we need to do is to set up the components in the __init__ method, and implement the compute_loss method, taking in the raw features and returning a loss value.\n",
        "\n",
        "The base model will then take care of creating the appropriate training loop to fit our model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8LTp0wDXX4F"
      },
      "source": [
        "#\n",
        "class MovielensModel(tfrs.Model):\n",
        "\n",
        "  def __init__(self, user_model, movie_model):\n",
        "    super().__init__()\n",
        "    self.movie_model: tf.keras.Model = movie_model\n",
        "    self.user_model: tf.keras.Model = user_model\n",
        "    self.task: tf.keras.layers.Layer = task\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    # We pick out the user features and pass them into the user model.\n",
        "    user_embeddings = self.user_model(features[\"user_id\"])\n",
        "    # And pick out the movie features and pass them into the movie model,\n",
        "    # getting embeddings back.\n",
        "    positive_movie_embeddings = self.movie_model(features[\"movie_title\"])\n",
        "\n",
        "    # The task computes the loss and the metrics.\n",
        "    return self.task(user_embeddings, positive_movie_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-bAjZXmXi3J"
      },
      "source": [
        "The tfrs.Model base class is a simply convenience class: it allows us to compute both training and test losses using the same method.\n",
        "\n",
        "Under the hood, it's still a plain Keras model. You could achieve the same functionality by inheriting from tf.keras.Model and overriding the train_step and test_step functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6dkppssfjtF"
      },
      "source": [
        "## Fitting and evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y0SYRepfkpf"
      },
      "source": [
        "#first instantiate the model\n",
        "model = MovielensModel(user_model, movie_model)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YK5RiWF1f1yH"
      },
      "source": [
        "train #from ratings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7Yvu6s3fo4T"
      },
      "source": [
        "#Then shuffle, batch, and cache the training and evaluation data.\n",
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v-CDR7ILgAXP"
      },
      "source": [
        "#train the model\n",
        "history=model.fit(cached_train, epochs=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PqCK1Riggpb"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjvCGTJ0gvOk"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_validation_runs = len(history.history[\"factorized_top_k/top_100_categorical_accuracy\"])\n",
        "epochs = [x for x in range(num_validation_runs)]\n",
        "\n",
        "plt.plot(epochs, history.history[\"factorized_top_k/top_100_categorical_accuracy\"], label=\"Top 100\")\n",
        "plt.plot(epochs, history.history[\"factorized_top_k/top_50_categorical_accuracy\"], label=\"Top 50\")\n",
        "plt.plot(epochs, history.history[\"factorized_top_k/top_10_categorical_accuracy\"], label=\"Top 10\")\n",
        "plt.title(\"Accuracy vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Top-K accuracy\");\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "geoo2ii1iIZh"
      },
      "source": [
        "As the model trains, the loss is falling and a set of top-k retrieval metrics is updated. These tell us whether the true positive is in the top-k retrieved items from the entire candidate set. For example, a top-5 categorical accuracy metric of 0.2 would tell us that, on average, the true positive is in the top 5 retrieved items 20% of the time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iejM8crhram"
      },
      "source": [
        "plt.plot(epochs, history.history[\"loss\"], label=\"loss\")\n",
        "plt.plot(epochs, history.history[\"total_loss\"], label=\"total loss\")\n",
        "plt.title(\"Loss vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Loss\");\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy6i_W46hrkT"
      },
      "source": [
        "#Finally, we can evaluate our model on the test set:\n",
        "model.evaluate(cached_test, return_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHH2j0pQiX9C"
      },
      "source": [
        "## Making predictions\n",
        "Now that we have a model, we would like to be able to make predictions. We can use the tfrs.layers.factorized_top_k.BruteForce layer to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9e_DS7VEhrnO"
      },
      "source": [
        "# Create a model that takes in raw query features, and\n",
        "index = tfrs.layers.factorized_top_k.BruteForce(model.user_model)\n",
        "# recommends movies out of the entire movies dataset.\n",
        "index.index(movies.batch(100).map(model.movie_model), movies)\n",
        "\n",
        "# Get recommendations.\n",
        "_, titles = index(tf.constant([\"42\"]))\n",
        "print(f\"Recommendations for user 42: {titles[0, :3]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NBWR8THFiiBX"
      },
      "source": [
        "the BruteForce layer is going to be too slow to serve a model with many possible candidates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iEWTzGYkN9q"
      },
      "source": [
        "# Ranking\n",
        "The ranking stage takes the outputs of the retrieval model and fine-tunes them to select the best possible handful of recommendations. Its task is to narrow down the set of items the user may be interested in to a shortlist of likely candidates."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYAjJJywkRMs"
      },
      "source": [
        "import os\n",
        "import pprint\n",
        "import tempfile\n",
        "\n",
        "from typing import Dict, Text\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow_recommenders as tfrs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HfZg0sykXUu"
      },
      "source": [
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "\n",
        "#we're also going to keep the ratings: these are the objectives we are trying to predict.\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"user_rating\": x[\"user_rating\"]\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZstIUShkdgz"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un-1Ra9JkgYG"
      },
      "source": [
        "movie_titles = ratings.batch(1_000_000).map(lambda x: x[\"movie_title\"])\n",
        "user_ids = ratings.batch(1_000_000).map(lambda x: x[\"user_id\"])\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movie_titles)))\n",
        "unique_user_ids = np.unique(np.concatenate(list(user_ids)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfHAbu63kkKU"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFdxYegHkmTw"
      },
      "source": [
        "#This model takes user ids and movie titles, and outputs a predicted rating\n",
        "class RankingModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    embedding_dimension = 32\n",
        "\n",
        "    # Compute embeddings for users.\n",
        "    self.user_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary=unique_user_ids, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_user_ids) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Compute embeddings for movies.\n",
        "    self.movie_embeddings = tf.keras.Sequential([\n",
        "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "        vocabulary=unique_movie_titles, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, embedding_dimension)\n",
        "    ])\n",
        "\n",
        "    # Compute predictions.\n",
        "    self.ratings = tf.keras.Sequential([\n",
        "      # Learn multiple dense layers.\n",
        "      tf.keras.layers.Dense(256, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(64, activation=\"relu\"),\n",
        "      # Make rating predictions in the final layer.\n",
        "      tf.keras.layers.Dense(1)\n",
        "  ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    user_id, movie_title = inputs\n",
        "\n",
        "    user_embedding = self.user_embeddings(user_id)\n",
        "    movie_embedding = self.movie_embeddings(movie_title)\n",
        "\n",
        "    return self.ratings(tf.concat([user_embedding, movie_embedding], axis=1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WF7mAsUOkx4s"
      },
      "source": [
        "#outputs a predicted rating\n",
        "RankingModel()(([\"42\"], [\"One Flew Over the Cuckoo's Nest (1975)\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K097kBtQk4P0"
      },
      "source": [
        "## Loss and metrics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnTY1tLbk5cT"
      },
      "source": [
        "#use the MeanSquaredError Keras loss in order to predict the ratings\n",
        "task = tfrs.tasks.Ranking(\n",
        "  loss = tf.keras.losses.MeanSquaredError(),\n",
        "  metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JvYz6lJalB_Z"
      },
      "source": [
        "## The full model\n",
        "TFRS exposes a base model class (tfrs.models.Model) which streamlines bulding models: all we need to do is to set up the components in the __init__ method, and implement the compute_loss method, taking in the raw features and returning a loss value."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEeKzXwBlGcE"
      },
      "source": [
        "class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.ranking_model: tf.keras.Model = RankingModel()\n",
        "    self.task: tf.keras.layers.Layer = tfrs.tasks.Ranking(\n",
        "      loss = tf.keras.losses.MeanSquaredError(),\n",
        "      metrics=[tf.keras.metrics.RootMeanSquaredError()]\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features: Dict[Text, tf.Tensor], training=False) -> tf.Tensor:\n",
        "    rating_predictions = self.ranking_model(\n",
        "        (features[\"user_id\"], features[\"movie_title\"]))\n",
        "\n",
        "    # The task computes the loss and the metrics.\n",
        "    return self.task(labels=features[\"user_rating\"], predictions=rating_predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzisOhm2lN5v"
      },
      "source": [
        "model = MovielensModel()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(learning_rate=0.1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEC28uShlQeN"
      },
      "source": [
        "cached_train = train.shuffle(100_000).batch(8192).cache()\n",
        "cached_test = test.batch(4096).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpW63NaelSPi"
      },
      "source": [
        "history=model.fit(cached_train, epochs=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Di5NowglWra"
      },
      "source": [
        "history.history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84bfOAoulg4l"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "num_validation_runs = len(history.history[\"loss\"])\n",
        "epochs = [x for x in range(num_validation_runs)]\n",
        "\n",
        "plt.plot(epochs, history.history[\"loss\"], label=\"Loss\")\n",
        "plt.plot(epochs, history.history[\"root_mean_squared_error\"], label=\"RMSE\")\n",
        "plt.title(\"Loss/RMSE vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"RMSE\");\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5soVTIGlydZ"
      },
      "source": [
        "model.evaluate(cached_test, return_dict=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlQhJP8rl238"
      },
      "source": [
        "The lower the RMSE metric, the more accurate our model is at predicting ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuGzMXLEnPWP"
      },
      "source": [
        "# Use more features\n",
        "One of the great advantages of using a deep learning framework to build recommender models is the freedom to build rich, flexible feature representations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z25FzOMnnbxi"
      },
      "source": [
        "import pprint\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "\n",
        "for x in ratings.take(1).as_numpy_iterator():\n",
        "  pprint.pprint(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIIThP5InftE"
      },
      "source": [
        "There are a couple of key features here:\n",
        "\n",
        "Movie title is useful as a movie identifier.\n",
        "User id is useful as a user identifier.\n",
        "Timestamps will allow us to model the effect of time.\n",
        "The first two are categorical features; timestamps are a continuous feature.\n",
        "\n",
        "Taking raw categorical features and turning them into embeddings is normally a two-step process:\n",
        "\n",
        "Firstly, we need to translate the raw values into a range of contiguous integers, normally by building a mapping (called a \"vocabulary\") that maps raw values (\"Star Wars\") to integers (say, 15).\n",
        "Secondly, we need to take these integers and turn them into embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb90cd5DoWTI"
      },
      "source": [
        "#The first step is to define a vocabulary\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "movie_title_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67HppIX5oPJz"
      },
      "source": [
        "movie_title_lookup.adapt(ratings.map(lambda x: x[\"movie_title\"]))\n",
        "\n",
        "print(f\"Vocabulary: {movie_title_lookup.get_vocabulary()[:3]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCYHWVOIoee5"
      },
      "source": [
        "#Once we have this we can use the layer to translate raw tokens to embedding ids:\n",
        "movie_title_lookup([\"Star Wars (1977)\", \"One Flew Over the Cuckoo's Nest (1975)\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YAxDW5tkpO-G"
      },
      "source": [
        "An embedding layer has two dimensions: the first dimension tells us how many distinct categories we can embed; the second tells us how large the vector representing each of them can be.\n",
        "\n",
        "When creating the embedding layer for movie titles, we are going to set the first value to the size of our title vocabulary (or the number of hashing bins). The second is up to us: the larger it is, the higher the capacity of the model, but the slower it is to fit and serve."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QKkdpfRqpPfl"
      },
      "source": [
        "movie_title_embedding = tf.keras.layers.Embedding(\n",
        "    # Let's use the explicit vocabulary lookup.\n",
        "    input_dim=movie_title_lookup.vocab_size(),\n",
        "    output_dim=32\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nDp8IPVBnRS_"
      },
      "source": [
        "#We can put the two together into a single layer which takes raw text in and yields embeddings.\n",
        "movie_title_model = tf.keras.Sequential([movie_title_lookup, movie_title_embedding])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwARQpCtl3w3"
      },
      "source": [
        "#test embedding\n",
        "movie_title_model([\"Star Wars (1977)\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gs0K2CL6pn2M"
      },
      "source": [
        "#We can do the same with user embeddings:\n",
        "user_id_lookup = tf.keras.layers.experimental.preprocessing.StringLookup()\n",
        "user_id_lookup.adapt(ratings.map(lambda x: x[\"user_id\"]))\n",
        "\n",
        "user_id_embedding = tf.keras.layers.Embedding(user_id_lookup.vocab_size(), 32)\n",
        "\n",
        "user_id_model = tf.keras.Sequential([user_id_lookup, user_id_embedding])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "arPiPzahpvP5"
      },
      "source": [
        "Normalizing continuous features, the timestamp feature is far too large to be used directly in a deep model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy5hMBikpvzJ"
      },
      "source": [
        "for x in ratings.take(10).as_numpy_iterator():\n",
        "  print(f\"Timestamp: {x['timestamp']}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA3YEO1lp8wd"
      },
      "source": [
        "Standardization rescales features to normalize their range by subtracting the feature's mean and dividing by its standard deviation. It is a common preprocessing transformation. This can be easily accomplished using the tf.keras.layers.experimental.preprocessing.Normalization layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uGdwZPcip_4D"
      },
      "source": [
        "timestamp_normalization = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "timestamp_normalization.adapt(ratings.map(lambda x: x[\"timestamp\"]).batch(1024))\n",
        "\n",
        "for x in ratings.take(3).as_numpy_iterator():\n",
        "  print(f\"Normalized timestamp: {timestamp_normalization(x['timestamp'])}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oCjS-QAqGaG"
      },
      "source": [
        "Another common transformation is to turn a continuous feature into a number of categorical features. To do this, we first need to establish the boundaries of the buckets we will use for discretization. The easiest way is to identify the minimum and maximum value of the feature, and divide the resulting interval equally:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeVSBsGhqKDh"
      },
      "source": [
        "max_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
        "    tf.cast(0, tf.int64), tf.maximum).numpy().max()\n",
        "min_timestamp = ratings.map(lambda x: x[\"timestamp\"]).reduce(\n",
        "    np.int64(1e9), tf.minimum).numpy().min()\n",
        "\n",
        "timestamp_buckets = np.linspace(\n",
        "    min_timestamp, max_timestamp, num=1000)\n",
        "\n",
        "print(f\"Buckets: {timestamp_buckets[:3]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6STzuMiqOd6"
      },
      "source": [
        "#Given the bucket boundaries we can transform timestamps into embeddings:\n",
        "timestamp_embedding_model = tf.keras.Sequential([\n",
        "  tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
        "  tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32)\n",
        "])\n",
        "\n",
        "for timestamp in ratings.take(1).map(lambda x: x[\"timestamp\"]).batch(1).as_numpy_iterator():\n",
        "  print(f\"Timestamp embedding: {timestamp_embedding_model(timestamp)}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1jEhLtgqbP8"
      },
      "source": [
        "Processing text features\n",
        "While the MovieLens dataset does not give us rich textual features, we can still use movie titles. This may help us capture the fact that movies with very similar titles are likely to belong to the same series.\n",
        "\n",
        "The first transformation we need to apply to text is tokenization (splitting into constituent words or word-pieces), followed by vocabulary learning, followed by an embedding.\n",
        "\n",
        "The Keras tf.keras.layers.experimental.preprocessing.TextVectorization layer can do the first two steps for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HP-I7sbZqRHp"
      },
      "source": [
        "title_text = tf.keras.layers.experimental.preprocessing.TextVectorization()\n",
        "title_text.adapt(ratings.map(lambda x: x[\"movie_title\"]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LC2E1HkEqRKT"
      },
      "source": [
        "for row in ratings.batch(1).map(lambda x: x[\"movie_title\"]).take(1):\n",
        "  print(title_text(row))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ehjCp1rqyIz"
      },
      "source": [
        "Each title is translated into a sequence of tokens, one for each piece we've tokenized.\n",
        "\n",
        "We can check the learned vocabulary to verify that the layer is using the correct tokenization:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAbO-Sp7qohr"
      },
      "source": [
        "title_text.get_vocabulary()[40:45]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zynsfgSbq3MM"
      },
      "source": [
        "## User model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m9NkgKWwqokJ"
      },
      "source": [
        "class UserModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.user_embedding = tf.keras.Sequential([\n",
        "        user_id_lookup,\n",
        "        tf.keras.layers.Embedding(user_id_lookup.vocab_size(), 32),\n",
        "    ])\n",
        "    self.timestamp_embedding = tf.keras.Sequential([\n",
        "      tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
        "      tf.keras.layers.Embedding(len(timestamp_buckets) + 2, 32)\n",
        "    ])\n",
        "    self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "\n",
        "  def call(self, inputs):\n",
        "\n",
        "    # Take the input dictionary, pass it through each input layer,\n",
        "    # and concatenate the result.\n",
        "    return tf.concat([\n",
        "        self.user_embedding(inputs[\"user_id\"]),\n",
        "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
        "        self.normalized_timestamp(inputs[\"timestamp\"])\n",
        "    ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQQL5UciqonH"
      },
      "source": [
        "user_model = UserModel()\n",
        "\n",
        "user_model.normalized_timestamp.adapt(\n",
        "    ratings.map(lambda x: x[\"timestamp\"]).batch(128))\n",
        "\n",
        "for row in ratings.batch(1).take(1):\n",
        "  print(f\"Computed representations: {user_model(row)[0, :3]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erhSY8Caq-sZ"
      },
      "source": [
        "## Movie Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "158XqImWq9mj"
      },
      "source": [
        "class MovieModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    max_tokens = 10_000\n",
        "\n",
        "    self.title_embedding = tf.keras.Sequential([\n",
        "      movie_title_lookup,\n",
        "      tf.keras.layers.Embedding(movie_title_lookup.vocab_size(), 32)\n",
        "    ])\n",
        "    self.title_text_embedding = tf.keras.Sequential([\n",
        "      tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=max_tokens),\n",
        "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
        "      # We average the embedding of individual words to get one embedding vector\n",
        "      # per title.\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    ])\n",
        "\n",
        "  def call(self, inputs):\n",
        "    return tf.concat([\n",
        "        self.title_embedding(inputs[\"movie_title\"]),\n",
        "        self.title_text_embedding(inputs[\"movie_title\"]),\n",
        "    ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNyOq9u2rBne"
      },
      "source": [
        "movie_model = MovieModel()\n",
        "\n",
        "movie_model.title_text_embedding.layers[0].adapt(\n",
        "    ratings.map(lambda x: x[\"movie_title\"]))\n",
        "\n",
        "for row in ratings.batch(1).take(1):\n",
        "  print(f\"Computed representations: {movie_model(row)[0, :3]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HFq_vvGrq61"
      },
      "source": [
        "## Build the model to use more features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6dO7LutsTYx"
      },
      "source": [
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
        "\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"timestamp\": x[\"timestamp\"],\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wfGsMn9ruHc"
      },
      "source": [
        "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
        "\n",
        "max_timestamp = timestamps.max()\n",
        "min_timestamp = timestamps.min()\n",
        "\n",
        "timestamp_buckets = np.linspace(\n",
        "    min_timestamp, max_timestamp, num=1000,\n",
        ")\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
        "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
        "    lambda x: x[\"user_id\"]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1S7PldJfrzq0"
      },
      "source": [
        "class UserModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self, use_timestamps):\n",
        "    super().__init__()\n",
        "\n",
        "    self._use_timestamps = use_timestamps\n",
        "\n",
        "    self.user_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=unique_user_ids, mask_token=None),\n",
        "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
        "    ])\n",
        "\n",
        "    if use_timestamps:\n",
        "      self.timestamp_embedding = tf.keras.Sequential([\n",
        "          tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
        "          tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
        "      ])\n",
        "      self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "\n",
        "      self.normalized_timestamp.adapt(timestamps)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    if not self._use_timestamps:\n",
        "      return self.user_embedding(inputs[\"user_id\"])\n",
        "\n",
        "    return tf.concat([\n",
        "        self.user_embedding(inputs[\"user_id\"]),\n",
        "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
        "        self.normalized_timestamp(inputs[\"timestamp\"]),\n",
        "    ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9n7ZeW2Vsas2"
      },
      "source": [
        "class MovieModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    max_tokens = 10_000\n",
        "\n",
        "    self.title_embedding = tf.keras.Sequential([\n",
        "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "          vocabulary=unique_movie_titles, mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
        "    ])\n",
        "\n",
        "    self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=max_tokens)#The maximum size of the vocabulary for this layer.\n",
        "\n",
        "    self.title_text_embedding = tf.keras.Sequential([\n",
        "      self.title_vectorizer,\n",
        "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    ])\n",
        "\n",
        "    self.title_vectorizer.adapt(movies)\n",
        "\n",
        "  def call(self, titles):\n",
        "    return tf.concat([\n",
        "        self.title_embedding(titles),\n",
        "        self.title_text_embedding(titles),\n",
        "    ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McAubdX5sfC6"
      },
      "source": [
        "With both UserModel and MovieModel defined, we can put together a combined model and implement our loss and metrics logic.\n",
        "\n",
        "Note that we also need to make sure that the query model and candidate model output embeddings of compatible size. Because we'll be varying their sizes by adding more features, the easiest way to accomplish this is to use a dense projection layer after each model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PT_0zwnsfpv"
      },
      "source": [
        "class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, use_timestamps):\n",
        "    super().__init__()\n",
        "    self.query_model = tf.keras.Sequential([\n",
        "      UserModel(use_timestamps),\n",
        "      tf.keras.layers.Dense(32)\n",
        "    ])\n",
        "    self.candidate_model = tf.keras.Sequential([\n",
        "      MovieModel(),\n",
        "      tf.keras.layers.Dense(32)\n",
        "    ])\n",
        "    self.task = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=movies.batch(128).map(self.candidate_model),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    # We only pass the user id and timestamp features into the query model. This\n",
        "    # is to ensure that the training inputs would have the same keys as the\n",
        "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
        "    # error when loading the query model after saving it.\n",
        "    query_embeddings = self.query_model({\n",
        "        \"user_id\": features[\"user_id\"],\n",
        "        \"timestamp\": features[\"timestamp\"],\n",
        "    })\n",
        "    movie_embeddings = self.candidate_model(features[\"movie_title\"])\n",
        "\n",
        "    return self.task(query_embeddings, movie_embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MZnOiy2sjJl"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)\n",
        "\n",
        "cached_train = train.shuffle(100_000).batch(2048)\n",
        "cached_test = test.batch(4096).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFpYepLrsoiW"
      },
      "source": [
        "#Baseline: no timestamp features\n",
        "model = MovielensModel(use_timestamps=False)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "model.fit(cached_train, epochs=3)\n",
        "\n",
        "train_accuracy = model.evaluate(\n",
        "    cached_train, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
        "test_accuracy = model.evaluate(\n",
        "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
        "\n",
        "print(f\"Top-100 accuracy (train): {train_accuracy:.2f}.\")\n",
        "print(f\"Top-100 accuracy (test): {test_accuracy:.2f}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrdHPHHKstl5"
      },
      "source": [
        "#Capturing time dynamics with time features\n",
        "model = MovielensModel(use_timestamps=True)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "model.fit(cached_train, epochs=3)\n",
        "\n",
        "train_accuracy = model.evaluate(\n",
        "    cached_train, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
        "test_accuracy = model.evaluate(\n",
        "    cached_test, return_dict=True)[\"factorized_top_k/top_100_categorical_accuracy\"]\n",
        "\n",
        "print(f\"Top-100 accuracy (train): {train_accuracy:.2f}.\")\n",
        "print(f\"Top-100 accuracy (test): {test_accuracy:.2f}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "23oRcNuWrK-R"
      },
      "source": [
        "# Deeper model with multiple stacked dense layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBKN9oYU6iU9"
      },
      "source": [
        "import os\n",
        "import tempfile\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "import tensorflow_recommenders as tfrs\n",
        "\n",
        "plt.style.use('seaborn-whitegrid')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VADklTH16pFf"
      },
      "source": [
        "ratings = tfds.load(\"movielens/100k-ratings\", split=\"train\")\n",
        "movies = tfds.load(\"movielens/100k-movies\", split=\"train\")\n",
        "\n",
        "ratings = ratings.map(lambda x: {\n",
        "    \"movie_title\": x[\"movie_title\"],\n",
        "    \"user_id\": x[\"user_id\"],\n",
        "    \"timestamp\": x[\"timestamp\"],\n",
        "})\n",
        "movies = movies.map(lambda x: x[\"movie_title\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4WHF4YN6u3A"
      },
      "source": [
        "timestamps = np.concatenate(list(ratings.map(lambda x: x[\"timestamp\"]).batch(100)))\n",
        "\n",
        "max_timestamp = timestamps.max()\n",
        "min_timestamp = timestamps.min()\n",
        "\n",
        "timestamp_buckets = np.linspace(\n",
        "    min_timestamp, max_timestamp, num=1000,\n",
        ")\n",
        "\n",
        "unique_movie_titles = np.unique(np.concatenate(list(movies.batch(1000))))\n",
        "unique_user_ids = np.unique(np.concatenate(list(ratings.batch(1_000).map(\n",
        "    lambda x: x[\"user_id\"]))))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fu5IR2F6yrv"
      },
      "source": [
        "## Query model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpRsP5dr6zk6"
      },
      "source": [
        "class UserModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.user_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "            vocabulary=unique_user_ids, mask_token=None),\n",
        "        tf.keras.layers.Embedding(len(unique_user_ids) + 1, 32),\n",
        "    ])\n",
        "    self.timestamp_embedding = tf.keras.Sequential([\n",
        "        tf.keras.layers.experimental.preprocessing.Discretization(timestamp_buckets.tolist()),\n",
        "        tf.keras.layers.Embedding(len(timestamp_buckets) + 1, 32),\n",
        "    ])\n",
        "    self.normalized_timestamp = tf.keras.layers.experimental.preprocessing.Normalization()\n",
        "\n",
        "    self.normalized_timestamp.adapt(timestamps)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    # Take the input dictionary, pass it through each input layer,\n",
        "    # and concatenate the result.\n",
        "    return tf.concat([\n",
        "        self.user_embedding(inputs[\"user_id\"]),\n",
        "        self.timestamp_embedding(inputs[\"timestamp\"]),\n",
        "        self.normalized_timestamp(inputs[\"timestamp\"]),\n",
        "    ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rBjw6by7THN"
      },
      "source": [
        "class QueryModel(tf.keras.Model):\n",
        "  \"\"\"Model for encoding user queries.\"\"\"\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "    \"\"\"Model for encoding user queries.\n",
        "\n",
        "    Args:\n",
        "      layer_sizes:\n",
        "        A list of integers where the i-th entry represents the number of units\n",
        "        the i-th layer contains.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    # We first use the user model for generating embeddings.\n",
        "    self.embedding_model = UserModel()\n",
        "\n",
        "    # Then construct the layers.\n",
        "    self.dense_layers = tf.keras.Sequential()\n",
        "\n",
        "    # Use the ReLU activation for all but the last layer.\n",
        "    for layer_size in layer_sizes[:-1]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "\n",
        "    # No activation for the last layer.\n",
        "    for layer_size in layer_sizes[-1:]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    feature_embedding = self.embedding_model(inputs)\n",
        "    return self.dense_layers(feature_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxUyUby77iqW"
      },
      "source": [
        "## Candidate model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSDG10D_7kez"
      },
      "source": [
        "class MovieModel(tf.keras.Model):\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    max_tokens = 10_000\n",
        "\n",
        "    self.title_embedding = tf.keras.Sequential([\n",
        "      tf.keras.layers.experimental.preprocessing.StringLookup(\n",
        "          vocabulary=unique_movie_titles,mask_token=None),\n",
        "      tf.keras.layers.Embedding(len(unique_movie_titles) + 1, 32)\n",
        "    ])\n",
        "\n",
        "    self.title_vectorizer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "        max_tokens=max_tokens)\n",
        "\n",
        "    self.title_text_embedding = tf.keras.Sequential([\n",
        "      self.title_vectorizer,\n",
        "      tf.keras.layers.Embedding(max_tokens, 32, mask_zero=True),\n",
        "      tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    ])\n",
        "\n",
        "    self.title_vectorizer.adapt(movies)\n",
        "\n",
        "  def call(self, titles):\n",
        "    return tf.concat([\n",
        "        self.title_embedding(titles),\n",
        "        self.title_text_embedding(titles),\n",
        "    ], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zCXPkb2P7pex"
      },
      "source": [
        "class CandidateModel(tf.keras.Model):\n",
        "  \"\"\"Model for encoding movies.\"\"\"\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "    \"\"\"Model for encoding movies.\n",
        "\n",
        "    Args:\n",
        "      layer_sizes:\n",
        "        A list of integers where the i-th entry represents the number of units\n",
        "        the i-th layer contains.\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "\n",
        "    self.embedding_model = MovieModel()\n",
        "\n",
        "    # Then construct the layers.\n",
        "    self.dense_layers = tf.keras.Sequential()\n",
        "\n",
        "    # Use the ReLU activation for all but the last layer.\n",
        "    for layer_size in layer_sizes[:-1]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size, activation=\"relu\"))\n",
        "\n",
        "    # No activation for the last layer.\n",
        "    for layer_size in layer_sizes[-1:]:\n",
        "      self.dense_layers.add(tf.keras.layers.Dense(layer_size))\n",
        "\n",
        "  def call(self, inputs):\n",
        "    feature_embedding = self.embedding_model(inputs)\n",
        "    return self.dense_layers(feature_embedding)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdaQ_tgQ7xBW"
      },
      "source": [
        "## Combined model\n",
        "With both QueryModel and CandidateModel defined, we can put together a combined model and implement our loss and metrics logic. To make things simple, we'll enforce that the model structure is the same across the query and candidate models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZ3EePg37zKK"
      },
      "source": [
        "class MovielensModel(tfrs.models.Model):\n",
        "\n",
        "  def __init__(self, layer_sizes):\n",
        "    super().__init__()\n",
        "    self.query_model = QueryModel(layer_sizes)\n",
        "    self.candidate_model = CandidateModel(layer_sizes)\n",
        "    self.task = tfrs.tasks.Retrieval(\n",
        "        metrics=tfrs.metrics.FactorizedTopK(\n",
        "            candidates=movies.batch(128).map(self.candidate_model),\n",
        "        ),\n",
        "    )\n",
        "\n",
        "  def compute_loss(self, features, training=False):\n",
        "    # We only pass the user id and timestamp features into the query model. This\n",
        "    # is to ensure that the training inputs would have the same keys as the\n",
        "    # query inputs. Otherwise the discrepancy in input structure would cause an\n",
        "    # error when loading the query model after saving it.\n",
        "    query_embeddings = self.query_model({\n",
        "        \"user_id\": features[\"user_id\"],\n",
        "        \"timestamp\": features[\"timestamp\"],\n",
        "    })\n",
        "    movie_embeddings = self.candidate_model(features[\"movie_title\"])\n",
        "\n",
        "    return self.task(\n",
        "        query_embeddings, movie_embeddings, compute_metrics=not training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnjBq0Yg73Bg"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-RbONDE74uK"
      },
      "source": [
        "tf.random.set_seed(42)\n",
        "shuffled = ratings.shuffle(100_000, seed=42, reshuffle_each_iteration=False)\n",
        "\n",
        "train = shuffled.take(80_000)\n",
        "test = shuffled.skip(80_000).take(20_000)\n",
        "\n",
        "cached_train = train.shuffle(100_000).batch(2048)\n",
        "cached_test = test.batch(4096).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tEKkwYaH77Vo"
      },
      "source": [
        "#Shallow model\n",
        "num_epochs = 300\n",
        "\n",
        "model = MovielensModel([32])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "one_layer_history = model.fit(\n",
        "    cached_train,\n",
        "    validation_data=cached_test,\n",
        "    validation_freq=5,\n",
        "    epochs=num_epochs,\n",
        "    verbose=0)\n",
        "\n",
        "accuracy = one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
        "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRiodiRu8CsZ"
      },
      "source": [
        "#Deep model\n",
        "model = MovielensModel([64, 32])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "two_layer_history = model.fit(\n",
        "    cached_train,\n",
        "    validation_data=cached_test,\n",
        "    validation_freq=5,\n",
        "    epochs=num_epochs,\n",
        "    verbose=0)\n",
        "\n",
        "accuracy = two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
        "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZbnvL-p8KNq"
      },
      "source": [
        "num_validation_runs = len(one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"])\n",
        "epochs = [(x + 1)* 5 for x in range(num_validation_runs)]\n",
        "\n",
        "plt.plot(epochs, one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"1 layer\")\n",
        "plt.plot(epochs, two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"2 layers\")\n",
        "plt.title(\"Accuracy vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Top-100 accuracy\");\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY-Bs70_nfHB"
      },
      "source": [
        "Even early on in the training, the larger model has a clear and stable lead over the shallow model, suggesting that adding depth helps the model capture more nuanced relationships in the data.\n",
        "\n",
        "However, even deeper models are not necessarily better. The following model extends the depth to three layers:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ugs0JdMa8OiF"
      },
      "source": [
        "model = MovielensModel([128, 64, 32])\n",
        "model.compile(optimizer=tf.keras.optimizers.Adagrad(0.1))\n",
        "\n",
        "three_layer_history = model.fit(\n",
        "    cached_train,\n",
        "    validation_data=cached_test,\n",
        "    validation_freq=5,\n",
        "    epochs=num_epochs,\n",
        "    verbose=0)\n",
        "\n",
        "accuracy = three_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"][-1]\n",
        "print(f\"Top-100 accuracy: {accuracy:.2f}.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JAGeVZvx8QZx"
      },
      "source": [
        "plt.plot(epochs, one_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"1 layer\")\n",
        "plt.plot(epochs, two_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"2 layers\")\n",
        "plt.plot(epochs, three_layer_history.history[\"val_factorized_top_k/top_100_categorical_accuracy\"], label=\"3 layers\")\n",
        "plt.title(\"Accuracy vs epoch\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"Top-100 accuracy\");\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neJAJVwbReNd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}